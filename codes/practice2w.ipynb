{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data files into dataframes with separator \"|\" or \",\"\n",
    "bnc = pd.read_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/nsc2_edu_bnc.txt\", sep=\"|\")\n",
    "bnd = pd.read_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/nsc2_edu_bnd.txt\", sep=\"|\")\n",
    "g1e = pd.read_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/nsc2_edu_g1e.txt\", sep=\"|\", encoding='cp949')\n",
    "m20 = pd.read_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/nsc2_edu_m20.txt\", sep=\",\")\n",
    "\n",
    "# Correct birth year in bnd and convert to numeric\n",
    "bnd['BTH_YYYY'] = bnd['BTH_YYYY'].replace('1921LE', '1921').astype(int)\n",
    "# Add randomness to birth year\n",
    "np.random.seed(1234)\n",
    "bnd['BTH_YYYY'] = bnd['BTH_YYYY'] + np.random.choice(range(10), size=len(bnd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in 2006: 3000\n",
      "Number of people who received health checkups in 2006: 2496\n"
     ]
    }
   ],
   "source": [
    "### 1. 2006년 건강검진을 받은 사람들을 추출\n",
    "bnc_2006 = bnc[bnc['STD_YYYY'] == 2006]\n",
    "g1e_2006_2012 = g1e[['RN_INDI', 'G1E_BMI', 'G1E_BP_SYS', 'G1E_TOT_CHOL', 'G1E_FBS', 'G1E_HGB', 'Q_FHX_HTDZ', 'Q_FHX_STK']]\n",
    "print(f\"Number of people in 2006: {len(bnc_2006)}\")\n",
    "print(f\"Number of people who received health checkups in 2006: {len(g1e_2006_2012)}\")\n",
    "#Number of people in 2006: 3000\n",
    "#Number of people who received health checkups in 2006: 2496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without missing values: 1938\n",
      "Number of people without missing values: 1019\n",
      "Number of people without missing values after summarising: 1019\n",
      "Number of people after joining: 1019\n"
     ]
    }
   ],
   "source": [
    "### 2. 관심 변수 중 Missing value가 있는 사람들 제외\n",
    "coh_people = g1e_2006_2012.dropna()\n",
    "print(f\"Number of rows without missing values: {len(coh_people)}\")\n",
    "#Number of rows without missing values: 1938\n",
    "print(f\"Number of people without missing values: {len(coh_people['RN_INDI'].unique())}\")\n",
    "#Number of people without missing values: 1019\n",
    "\n",
    "# Note: g1e_2006_2012 테이블에는 한 사람당 검진 결과가 여러 번 있을 수 있다. 따라서 검진 결과를 summarise하여 한 사람당 한 행으로 만든다.\n",
    "# RN_INDI를 기준으로 관심 변수를 summarise\n",
    "# G1E_BMI -> mean\n",
    "# G1E_BP_SYS -> mean\n",
    "# G1E_TOT_CHOL -> mean\n",
    "# G1E_FBS -> mean\n",
    "# G1E_HGB -> mean\n",
    "# Q_FHX_HTDZ -> max\n",
    "# Q_FHX_STK -> max\n",
    "coh_people = coh_people.groupby('RN_INDI').agg({\n",
    "    'G1E_BMI': 'mean',\n",
    "    'G1E_BP_SYS': 'mean',\n",
    "    'G1E_TOT_CHOL': 'mean',\n",
    "    'G1E_FBS': 'mean',\n",
    "    'G1E_HGB': 'mean',\n",
    "    'Q_FHX_HTDZ': 'max',\n",
    "    'Q_FHX_STK': 'max'\n",
    "}).reset_index()\n",
    "print(f\"Number of people without missing values after summarising: {len(coh_people)}\")\n",
    "#Number of people without missing values after summarising: 1019\n",
    "\n",
    "# 심혈관질환 가족력 (FHX_CVD) 변수 생성 (Q_FHX_HTDZ == 2 or Q_FHX_STK == 2)\n",
    "coh_people['FHX_CVD'] = np.where((coh_people['Q_FHX_HTDZ'] == 2) | (coh_people['Q_FHX_STK'] == 2), 1, 0)\n",
    "# Q_FHX_HTDZ, Q_FHX_STK 변수 제거\n",
    "coh_people = coh_people.drop(columns=['Q_FHX_HTDZ', 'Q_FHX_STK'])\n",
    "\n",
    "# bnc_2006 데이터 (SEX)와 bnd 데이터 (BTH_YYYY)를 coh_people에 left_join\n",
    "coh_people = pd.merge(coh_people, bnc_2006[['RN_INDI', 'SEX']], on='RN_INDI', how='left')\n",
    "coh_people = pd.merge(coh_people, bnd[['RN_INDI', 'BTH_YYYY']], on='RN_INDI', how='left')\n",
    "print(f\"Number of people after joining: {len(coh_people)}\")\n",
    "#Number of people after joining: 1019\n",
    "# 나이 (AGE) 변수 생성\n",
    "coh_people['AGE'] = 2006 - coh_people['BTH_YYYY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people after merging HYP and ASCVD events: 1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47448/2694877838.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  m20_event = m20_event.groupby('RN_INDI').apply(lambda x: x.loc[x['MDCARE_STRT_DT'].idxmin()]).reset_index(drop=True)\n",
      "/tmp/ipykernel_47448/2694877838.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  m20_event2 = m20_event2.groupby('RN_INDI').apply(lambda x: x.loc[x['MDCARE_STRT_DT'].idxmin()]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# m20 데이터에서 고혈압 진단 코드 (I10)를 가진 사람들을 추출\n",
    "# event_code는 I10\n",
    "event_code = ['I10']\n",
    "# coh_people에 있는 사람들만 m20 데이터에서 추출\n",
    "m20_2 = m20[m20['RN_INDI'].isin(coh_people['RN_INDI'])]\n",
    "# m20_2의 SICK_SYM1 또는 SICK_SYM2 에서 event_code를 포함하는 사람들을 추출\n",
    "m20_event = m20_2[m20_2[['SICK_SYM1', 'SICK_SYM2']].apply(lambda x: x.str.contains('|'.join(event_code))).any(axis=1)]\n",
    "# RN_INDI를 기준으로 가장 빠른 고혈압 진단일을 선택\n",
    "m20_event = m20_event.groupby('RN_INDI').apply(lambda x: x.loc[x['MDCARE_STRT_DT'].idxmin()]).reset_index(drop=True)\n",
    "# HYP 변수 생성\n",
    "m20_event['HYP'] = 1\n",
    "\n",
    "# m20 데이터에서 ASCVD 진단 코드를 가진 사람들을 추출\n",
    "# event2_code는 I20~I25, G45, G46, I63, I64, I65, I66, I672, I694, I70, I738, I739\n",
    "event2_code = ['I' + str(i) for i in range(20, 26)] + ['G45', 'G46', 'I63', 'I64', 'I65', 'I66', 'I672', 'I694', 'I70', 'I738', 'I739']\n",
    "# m20_2의 SICK_SYM1 또는 SICK_SYM2 에서 event2_code를 포함하는 사람들을 추출\n",
    "m20_event2 = m20_2[m20_2[['SICK_SYM1', 'SICK_SYM2']].apply(lambda x: x.str.contains('|'.join(event2_code))).any(axis=1)]\n",
    "# RN_INDI를 기준으로 가장 빠른 ASCVD 진단일을 선택\n",
    "m20_event2 = m20_event2.groupby('RN_INDI').apply(lambda x: x.loc[x['MDCARE_STRT_DT'].idxmin()]).reset_index(drop=True)\n",
    "# ASCVD 변수 생성\n",
    "m20_event2['ASCVD'] = 1\n",
    "\n",
    "# coh_people에 HYP, MDCARE_STRT_DT(rename - HYP_DATE) left_join하고 \n",
    "coh_people = pd.merge(coh_people, m20_event[['RN_INDI', 'HYP', 'MDCARE_STRT_DT']].rename(columns={'MDCARE_STRT_DT': 'HYP_DATE'}), on='RN_INDI', how='left')\n",
    "# HYP 변수의 missing value를 0으로 채움\n",
    "coh_people['HYP'] = coh_people['HYP'].fillna(0)\n",
    "# coh_people에 ASCVD, MDCARE_STRT_DT(rename - ASCVD_DATE) left_join \n",
    "coh_people = pd.merge(coh_people, m20_event2[['RN_INDI', 'ASCVD', 'MDCARE_STRT_DT']].rename(columns={'MDCARE_STRT_DT': 'ASCVD_DATE'}), on='RN_INDI', how='left')\n",
    "# ASCVD 변수의 missing value를 0으로 채움\n",
    "coh_people['ASCVD'] = coh_people['ASCVD'].fillna(0)\n",
    "\n",
    "# 3. 최종 인원 선택\n",
    "print(f\"Number of people after merging HYP and ASCVD events: {len(coh_people)}\")\n",
    "\n",
    "# Add some randomness to RN_INDI for privacy protection\n",
    "np.random.seed(1234)\n",
    "coh_people['RN_INDI'] = coh_people['RN_INDI'] + np.random.choice(range(1, 101), size=len(coh_people))\n",
    "# Select and reorder columns\n",
    "coh_people = coh_people[['RN_INDI', 'SEX', 'BTH_YYYY', 'AGE', 'G1E_BMI', 'G1E_BP_SYS', 'G1E_FBS', 'G1E_HGB', 'G1E_TOT_CHOL', 'FHX_CVD', 'HYP', 'HYP_DATE', 'ASCVD', 'ASCVD_DATE']]\n",
    "# Save the cohort data to a CSV file\n",
    "coh_people.to_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/hyp_ascvd_cohort.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train: 815\n",
      "Number of rows in val: 101\n",
      "Number of rows in test: 103\n"
     ]
    }
   ],
   "source": [
    "# 4. train, val, test로 구분 (8:1:1)\n",
    "num_train = int(len(coh_people) * 0.8)\n",
    "num_val = int(len(coh_people) * 0.1)\n",
    "num_test = len(coh_people) - num_train - num_val\n",
    "\n",
    "# randomly select indices for train, val, test\n",
    "np.random.seed(1234)\n",
    "indices = np.random.permutation(len(coh_people))\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:num_train+num_val]\n",
    "test_indices = indices[num_train+num_val:]\n",
    "\n",
    "# split the dataset\n",
    "train = coh_people.iloc[train_indices].reset_index(drop=True)\n",
    "val = coh_people.iloc[val_indices].reset_index(drop=True)\n",
    "test = coh_people.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "# print number of rows\n",
    "print(f\"Number of rows in train: {len(train)}\")\n",
    "print(f\"Number of rows in val: {len(val)}\")\n",
    "print(f\"Number of rows in test: {len(test)}\")\n",
    "#Number of rows in train: 815\n",
    "#Number of rows in val: 101\n",
    "#Number of rows in test: 103\n",
    "\n",
    "# save the dataset\n",
    "train.to_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/hyp_ascvd_cohort_train.csv\", index=False)\n",
    "val.to_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/hyp_ascvd_cohort_val.csv\", index=False)\n",
    "test.to_csv(\"/mnt/c/AI_practice_20250314/data/nhis_edu/hyp_ascvd_cohort_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without missing values: 1938\n",
      "Number of people without missing values: 1019\n",
      "Number of people without missing values after summarising: 1019\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'STND_Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yc_torch/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'STND_Y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m coh_people = coh_people.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mQ_FHX_HTDZ\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mQ_FHX_STK\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# bnc 데이터에서 2006년 데이터 필터링\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m bnc_2006 = bnc[\u001b[43mbnc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSTND_Y\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[32m2006\u001b[39m]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# bnc_2006 데이터 (SEX)와 bnd 데이터 (BTH_YYYY)를 coh_people에 left_join\u001b[39;00m\n\u001b[32m     40\u001b[39m coh_people = pd.merge(coh_people, bnc_2006[[\u001b[33m'\u001b[39m\u001b[33mRN_INDI\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSEX\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mRN_INDI\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yc_torch/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yc_torch/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'STND_Y'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
